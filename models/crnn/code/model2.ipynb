{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802f4f71",
   "metadata": {},
   "source": [
    "数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d202974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class DoorplateDataset(Dataset):\n",
    "    def __init__(self, data_dir, json_file, transform=None, max_seq_length=10, is_test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 读取标注文件\n",
    "        if not self.is_test:\n",
    "            with open(json_file, 'r') as f:\n",
    "                self.annotations = json.load(f)\n",
    "            self.image_files = list(self.annotations.keys())\n",
    "        else:\n",
    "            self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        \n",
    "        # 读取图像\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            # 获取标注信息\n",
    "            ann = self.annotations[img_name]\n",
    "            \n",
    "            # 构建目标数据\n",
    "            labels = ann['label']\n",
    "            lefts = ann['left']\n",
    "            tops = ann['top']\n",
    "            widths = ann['width']\n",
    "            heights = ann['height']\n",
    "            \n",
    "            # 创建目标序列和边界框\n",
    "            seq = np.zeros(self.max_seq_length, dtype=np.int64)\n",
    "            seq_len = min(len(labels), self.max_seq_length)\n",
    "            \n",
    "            # 填充序列\n",
    "            for i in range(seq_len):\n",
    "                seq[i] = labels[i]\n",
    "            \n",
    "            # 归一化边界框坐标\n",
    "            h, w = image.shape[:2]\n",
    "            bbox = np.array([lefts[0]/w, tops[0]/h, widths[0]/w, heights[0]/h], dtype=np.float32)\n",
    "                \n",
    "            # 应用变换\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "            \n",
    "            return image, seq, seq_len, bbox\n",
    "        \n",
    "        else:\n",
    "            # 测试集只返回图像\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "            \n",
    "            return image, img_name\n",
    "\n",
    "def get_transforms(is_train=True):\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=128, width=384),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1),\n",
    "                A.RandomGamma(),\n",
    "                A.GaussNoise(),\n",
    "            ], p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=128, width=384),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "def get_dataloader(data_dir, json_file, batch_size, is_train=True, is_test=False):\n",
    "    transform = get_transforms(is_train)\n",
    "    dataset = DoorplateDataset(data_dir, json_file, transform, is_test=is_test)\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_train,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f675031",
   "metadata": {},
   "source": [
    "工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        images, targets, target_lengths, bbox_targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        bbox_targets = bbox_targets.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        logits, bbox_pred = model(images)\n",
    "        \n",
    "        # 计算序列损失\n",
    "        batch_size, seq_len, num_classes = logits.size()\n",
    "        input_lengths = torch.full((batch_size,), seq_len, dtype=torch.long, device=device)\n",
    "        \n",
    "        # CTC损失\n",
    "        log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)  # (seq_len, batch, num_classes)\n",
    "        ctc_loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "        \n",
    "        # 定位损失 (使用MSE)\n",
    "        bbox_loss = F.mse_loss(bbox_pred, bbox_targets)\n",
    "        \n",
    "        # 总损失\n",
    "        loss = ctc_loss + bbox_loss\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 计算准确率\n",
    "        _, preds = logits.max(2)\n",
    "        for i in range(batch_size):\n",
    "            pred_str = ''.join([str(p.item()) for p in preds[i] if p != 0])\n",
    "            target_str = ''.join([str(targets[i][j].item()) for j in range(target_lengths[i].item())])\n",
    "            if pred_str == target_str:\n",
    "                correct += 1\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": loss.item(), \"accuracy\": correct / total_samples})\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total_samples\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "        for batch in pbar:\n",
    "            images, targets, target_lengths, bbox_targets = batch\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            bbox_targets = bbox_targets.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            logits, bbox_pred = model(images)\n",
    "            \n",
    "            # 计算序列损失\n",
    "            batch_size, seq_len, num_classes = logits.size()\n",
    "            input_lengths = torch.full((batch_size,), seq_len, dtype=torch.long, device=device)\n",
    "            \n",
    "            # CTC损失\n",
    "            log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)\n",
    "            ctc_loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            # 定位损失\n",
    "            bbox_loss = F.mse_loss(bbox_pred, bbox_targets)\n",
    "            \n",
    "            # 总损失\n",
    "            loss = ctc_loss + bbox_loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 计算准确率\n",
    "            _, preds = logits.max(2)\n",
    "            for i in range(batch_size):\n",
    "                pred_str = ''.join([str(p.item()) for p in preds[i] if p != 0])\n",
    "                target_str = ''.join([str(targets[i][j].item()) for j in range(target_lengths[i].item())])\n",
    "                if pred_str == target_str:\n",
    "                    correct += 1\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            pbar.set_postfix({\"val_loss\": loss.item(), \"val_accuracy\": correct / total_samples})\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total_samples\n",
    "\n",
    "def decode_predictions(logits, image_size):\n",
    "    \"\"\"从模型输出解码得到字符和位置\"\"\"\n",
    "    batch_size, seq_len, num_classes = logits.size()\n",
    "    \n",
    "    # 解码序列\n",
    "    _, preds = logits.max(2)\n",
    "    \n",
    "    # 移除重复字符和空白符（CTC解码）\n",
    "    decoded_preds = []\n",
    "    for i in range(batch_size):\n",
    "        pred = preds[i].cpu().numpy()\n",
    "        decoded = []\n",
    "        prev_char = -1\n",
    "        for p in pred:\n",
    "            if p != 0 and p != prev_char:  # 0通常是空白符\n",
    "                decoded.append(p)\n",
    "            prev_char = p\n",
    "        decoded_preds.append(decoded)\n",
    "    \n",
    "    return decoded_preds\n",
    "\n",
    "def predict(model, dataloader, device, output_dir):\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            images, img_names = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            logits, bbox_pred = model(images)\n",
    "            \n",
    "            # 解码预测\n",
    "            decoded = decode_predictions(logits, images.shape[-2:])\n",
    "            \n",
    "            # 将预测结果保存\n",
    "            for i, img_name in enumerate(img_names):\n",
    "                h, w = 128, 384  # 模型输入的图像大小\n",
    "                \n",
    "                # 获取原始图像大小\n",
    "                orig_img = cv2.imread(os.path.join(dataloader.dataset.data_dir, img_name))\n",
    "                orig_h, orig_w = orig_img.shape[:2]\n",
    "                \n",
    "                # 缩放边界框到原始图像大小\n",
    "                x, y, width, height = bbox_pred[i].cpu().numpy()\n",
    "                x = int(x * orig_w)\n",
    "                y = int(y * orig_h)\n",
    "                width = int(width * orig_w)\n",
    "                height = int(height * orig_h)\n",
    "                \n",
    "                # 保存预测结果\n",
    "                label = [int(c) for c in decoded[i]]\n",
    "                \n",
    "                if img_name not in predictions:\n",
    "                    predictions[img_name] = {\n",
    "                        \"label\": label,\n",
    "                        \"top\": [y],\n",
    "                        \"left\": [x],\n",
    "                        \"height\": [height],\n",
    "                        \"width\": [width]\n",
    "                    }\n",
    "                else:\n",
    "                    predictions[img_name][\"label\"].extend(label)\n",
    "                    predictions[img_name][\"top\"].extend([y])\n",
    "                    predictions[img_name][\"left\"].extend([x])\n",
    "                    predictions[img_name][\"height\"].extend([height])\n",
    "                    predictions[img_name][\"width\"].extend([width])\n",
    "    \n",
    "    # 保存预测结果\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(os.path.join(output_dir, \"predictions.json\"), \"w\") as f:\n",
    "        json.dump(predictions, f)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f2760",
   "metadata": {},
   "source": [
    "模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, rnn_hidden_size=256):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # 使用ResNet18作为骨干网络，提取特征\n",
    "        resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # 移除最后的池化和全连接层\n",
    "        \n",
    "        # 定位网络\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 1 * 3, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 4)  # 输出4个坐标值：x, y, width, height\n",
    "        )\n",
    "        \n",
    "        # 序列化特征\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "        \n",
    "        # LSTM层\n",
    "        self.rnn = nn.LSTM(512, rnn_hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # 分类层\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 特征提取\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # 定位\n",
    "        bbox = self.localization(features)\n",
    "        bbox = torch.sigmoid(bbox)  # 将输出限制在0-1范围内\n",
    "        \n",
    "        # 序列化特征\n",
    "        seq_features = self.avg_pool(features)\n",
    "        seq_features = seq_features.squeeze(2)\n",
    "        seq_features = seq_features.permute(0, 2, 1)  # [batch, width, channels]\n",
    "        \n",
    "        # RNN处理\n",
    "        rnn_out, _ = self.rnn(seq_features)\n",
    "        \n",
    "        # 分类\n",
    "        logits = self.classifier(rnn_out)\n",
    "        \n",
    "        return logits, bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec905ea",
   "metadata": {},
   "source": [
    "模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb16dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import CRNN\n",
    "from data_loader import get_dataloader\n",
    "from utils import train_epoch, evaluate\n",
    "\n",
    "def main():\n",
    "    # 配置\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_epochs = 100\n",
    "    batch_size = 32  # 增加批量大小\n",
    "    learning_rate = 0.0005  # 降低学习率\n",
    "    early_stop_patience = 15  # 增加早停耐心值\n",
    "    \n",
    "    # 数据集路径\n",
    "    base_dir = '../tcdata'\n",
    "    train_data_dir = os.path.join(base_dir, 'mchar_train')\n",
    "    train_json = os.path.join(base_dir, 'train.json')\n",
    "    val_data_dir = os.path.join(base_dir, 'mchar_val')\n",
    "    val_json = os.path.join(base_dir, 'mchar_val.json')\n",
    "    \n",
    "    # 模型保存路径\n",
    "    model_save_dir = '../user_data/model_data'\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    # 获取数据加载器\n",
    "    train_loader = get_dataloader(train_data_dir, train_json, batch_size, is_train=True)\n",
    "    val_loader = get_dataloader(val_data_dir, val_json, batch_size, is_train=False)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = CRNN(num_classes=11)  # 包括0-9和空白符\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CTCLoss(blank=10, reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # 添加权重衰减\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)  # 调整学习率调度\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0  # 记录验证损失没有改善的轮数\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # 训练\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        # 验证\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # 学习率调整\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc\n",
    "            }, os.path.join(model_save_dir, 'best_model.pth'))\n",
    "            print(\"保存最佳模型！\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "        \n",
    "        # 保存最后模型\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        }, os.path.join(model_save_dir, 'last_model.pth'))\n",
    "        \n",
    "        # 早停检查\n",
    "        if no_improve_epochs >= early_stop_patience:\n",
    "            print(f\"\\n早停：验证损失连续{early_stop_patience}轮没有改善\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n训练完成！\")\n",
    "    print(f\"最佳验证损失: {best_val_loss:.4f}\")\n",
    "    print(f\"最后模型保存在: {os.path.join(model_save_dir, 'last_model.pth')}\")\n",
    "    print(f\"最佳模型保存在: {os.path.join(model_save_dir, 'best_model.pth')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3869545",
   "metadata": {},
   "source": [
    "模型推理与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import CRNN\n",
    "from data_loader import get_transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_image_quality(image):\n",
    "    \"\"\"\n",
    "    检查图像质量\n",
    "    返回: (bool, str) - (是否通过检查, 失败原因)\n",
    "    \"\"\"\n",
    "    # 检查图像是否为空\n",
    "    if image is None:\n",
    "        return False, \"图像读取失败\"\n",
    "    \n",
    "    # 检查图像尺寸\n",
    "    h, w = image.shape[:2]\n",
    "    if h < 16 or w < 16:  # 降低最小尺寸要求\n",
    "        return False, f\"图像尺寸过小: {h}x{w}\"\n",
    "    \n",
    "    # 检查图像是否模糊\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    if laplacian < 30:  # 降低模糊检测阈值\n",
    "        return False, f\"图像模糊: {laplacian:.2f}\"\n",
    "    \n",
    "    # 检查图像亮度\n",
    "    brightness = np.mean(gray)\n",
    "    if brightness < 20:  # 降低暗度阈值\n",
    "        return False, f\"图像过暗: {brightness:.2f}\"\n",
    "    if brightness > 230:  # 提高亮度阈值\n",
    "        return False, f\"图像过亮: {brightness:.2f}\"\n",
    "    \n",
    "    # 检查图像对比度\n",
    "    contrast = np.std(gray)\n",
    "    if contrast < 20:  # 添加对比度检查\n",
    "        return False, f\"图像对比度过低: {contrast:.2f}\"\n",
    "    \n",
    "    return True, \"图像质量正常\"\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    预处理图像\n",
    "    返回: 预处理后的图像\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # 调整图像大小\n",
    "    h, w = image.shape[:2]\n",
    "    if h < 16 or w < 16:\n",
    "        scale = max(16/h, 16/w)\n",
    "        image = cv2.resize(image, (int(w*scale), int(h*scale)))\n",
    "    \n",
    "    # 调整亮度和对比度\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    brightness = np.mean(gray)\n",
    "    contrast = np.std(gray)\n",
    "    \n",
    "    # 如果图像过暗\n",
    "    if brightness < 20:\n",
    "        alpha = 1.5  # 增加亮度\n",
    "        beta = 30    # 增加偏移\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    # 如果图像过亮\n",
    "    elif brightness > 230:\n",
    "        alpha = 0.8  # 降低亮度\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=0)\n",
    "    \n",
    "    # 如果对比度过低\n",
    "    if contrast < 20:\n",
    "        # 使用直方图均衡化\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        gray = cv2.equalizeHist(gray)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, max_samples=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        # 获取所有图片文件并按数字顺序排序\n",
    "        self.image_files = []\n",
    "        for f in os.listdir(data_dir):\n",
    "            if f.endswith('.png') or f.endswith('.jpg'):\n",
    "                try:\n",
    "                    # 验证文件名格式\n",
    "                    num = int(f.split('.')[0])\n",
    "                    self.image_files.append(f)\n",
    "                except ValueError:\n",
    "                    print(f\"警告：跳过无效的文件名 {f}\")\n",
    "                    continue\n",
    "        self.image_files.sort(key=lambda x: int(x.split('.')[0]))\n",
    "        \n",
    "        if max_samples is not None:\n",
    "            self.image_files = self.image_files[:max_samples]\n",
    "        \n",
    "        # 创建debug目录\n",
    "        self.debug_dir = os.path.join(os.path.dirname(data_dir), 'debug_images')\n",
    "        os.makedirs(self.debug_dir, exist_ok=True)\n",
    "        \n",
    "        # 打印加载的图片数量\n",
    "        print(f\"成功加载 {len(self.image_files)} 张图片\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            # 读取图像\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"警告：无法读取图像 {img_name}\")\n",
    "                # 返回一个占位图像\n",
    "                image = np.zeros((128, 384, 3), dtype=np.uint8)\n",
    "                return image, img_name, (128, 384), False\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # 预处理图像\n",
    "            image = preprocess_image(image)\n",
    "            if image is None:\n",
    "                print(f\"警告：图像 {img_name} 预处理失败\")\n",
    "                image = np.zeros((128, 384, 3), dtype=np.uint8)\n",
    "                return image, img_name, (128, 384), False\n",
    "            \n",
    "            # 保存原始图像尺寸\n",
    "            orig_h, orig_w = image.shape[:2]\n",
    "            \n",
    "            # 检查图像质量\n",
    "            is_valid, reason = check_image_quality(image)\n",
    "            if not is_valid:\n",
    "                print(f\"警告：图像 {img_name} 质量检查未通过 - {reason}\")\n",
    "                # 保存问题图像用于调试\n",
    "                debug_path = os.path.join(self.debug_dir, f\"invalid_{img_name}\")\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "                # 尝试再次预处理\n",
    "                image = preprocess_image(image)\n",
    "                if image is not None:\n",
    "                    is_valid, _ = check_image_quality(image)\n",
    "            \n",
    "            # 应用变换\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "            \n",
    "            return image, img_name, (orig_h, orig_w), is_valid\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"处理图像 {img_name} 时出错: {str(e)}\")\n",
    "            # 返回一个占位图像\n",
    "            image = np.zeros((128, 384, 3), dtype=np.uint8)\n",
    "            return image, img_name, (128, 384), False\n",
    "\n",
    "def decode_predictions(pred, blank=10):\n",
    "    \"\"\"从CTC预测中解码标签\"\"\"\n",
    "    # 找到最可能的类别\n",
    "    _, max_indices = pred.max(2)\n",
    "    \n",
    "    # 将预测转换为numpy数组\n",
    "    max_indices = max_indices.cpu().numpy()\n",
    "    \n",
    "    batch_results = []\n",
    "    for indices in max_indices:\n",
    "        # 移除重复的元素\n",
    "        collapsed = []\n",
    "        previous = None\n",
    "        for idx in indices:\n",
    "            if idx != previous and idx != blank:\n",
    "                collapsed.append(int(idx))\n",
    "            previous = idx\n",
    "        batch_results.append(collapsed)\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def main():\n",
    "    # 配置\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_data_dir = '../tcdata/mchar_test_a'\n",
    "    model_path = '../user_data/model_data/best_model.pth'\n",
    "    output_dir = '../prediction_result'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 获取所有测试图片\n",
    "    all_image_files = sorted(\n",
    "        [f for f in os.listdir(test_data_dir) if f.endswith('.png')],\n",
    "        key=lambda x: int(x.split('.')[0])\n",
    "    )\n",
    "    print(f\"找到 {len(all_image_files)} 张测试图片\")\n",
    "    \n",
    "    # 检查模型文件\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"错误：模型文件 {model_path} 不存在\")\n",
    "        return\n",
    "    \n",
    "    # 转换\n",
    "    transform = A.Compose([\n",
    "        A.Resize(height=128, width=384),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n",
    "            A.RandomGamma(gamma_limit=(80, 120)),\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8)),\n",
    "        ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0)),\n",
    "            A.GaussianBlur(blur_limit=3),\n",
    "            A.MedianBlur(blur_limit=3),\n",
    "        ], p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    # 加载测试数据\n",
    "    print(\"加载测试数据...\")\n",
    "    test_dataset = TestDataset(test_data_dir, transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # 修改为0以避免多进程问题\n",
    "        pin_memory=True,\n",
    "        drop_last=False,  # 确保不丢弃最后一批\n",
    "        collate_fn=lambda x: (torch.stack([item[0] for item in x]),  # 图像\n",
    "                            [item[1] for item in x],  # 文件名\n",
    "                            [item[2] for item in x],  # 原始尺寸\n",
    "                            torch.tensor([item[3] for item in x]))  # 有效性标记\n",
    "    )\n",
    "    \n",
    "    # 加载模型\n",
    "    print(\"加载模型...\")\n",
    "    model = CRNN(num_classes=11)  # 包括0-9和空白符\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"成功加载最佳模型，验证损失: {checkpoint['val_loss']:.4f}, 验证准确率: {checkpoint['val_acc']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载模型时出错: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # 预测结果\n",
    "    predictions = {}\n",
    "    processed_files = set()\n",
    "    invalid_files = set()\n",
    "    error_files = set()\n",
    "    \n",
    "    print(\"开始预测...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, img_names, orig_sizes, is_valid) in enumerate(tqdm(test_loader, desc=\"Predicting\")):\n",
    "            try:\n",
    "                images = images.to(device)\n",
    "                \n",
    "                # 前向传播\n",
    "                logits, bbox_preds = model(images)\n",
    "                \n",
    "                # 确保模型输出与输入批次大小一致\n",
    "                batch_size = images.size(0)\n",
    "                if logits.size(0) != batch_size or bbox_preds.size(0) != batch_size:\n",
    "                    print(f\"警告：模型输出批次大小不一致，输入: {batch_size}, logits: {logits.size(0)}, bbox: {bbox_preds.size(0)}\")\n",
    "                    continue\n",
    "                \n",
    "                # 解码预测\n",
    "                batch_labels = decode_predictions(logits)\n",
    "                \n",
    "                # 确保所有数据长度一致\n",
    "                if len(batch_labels) != batch_size:\n",
    "                    print(f\"警告：解码后的标签数量不一致，预期: {batch_size}, 实际: {len(batch_labels)}\")\n",
    "                    continue\n",
    "                \n",
    "                # 处理每个图像的预测\n",
    "                for i in range(batch_size):\n",
    "                    try:\n",
    "                        img_name = img_names[i]\n",
    "                        labels = batch_labels[i]\n",
    "                        is_img_valid = is_valid[i]\n",
    "                        orig_size = orig_sizes[i]\n",
    "                        \n",
    "                        # 获取原始图像尺寸\n",
    "                        if isinstance(orig_size, tuple):\n",
    "                            orig_h, orig_w = orig_size\n",
    "                        else:\n",
    "                            orig_h, orig_w = orig_size[0], orig_size[1]\n",
    "                        \n",
    "                        # 预测的边界框\n",
    "                        bbox = bbox_preds[i].cpu().numpy()\n",
    "                        x = int(bbox[0] * orig_w)\n",
    "                        y = int(bbox[1] * orig_h)\n",
    "                        width = int(bbox[2] * orig_w)\n",
    "                        height = int(bbox[3] * orig_h)\n",
    "                        \n",
    "                        # 保存预测结果\n",
    "                        predictions[img_name] = {\n",
    "                            \"label\": labels,\n",
    "                            \"top\": [y],\n",
    "                            \"left\": [x],\n",
    "                            \"height\": [height],\n",
    "                            \"width\": [width]\n",
    "                        }\n",
    "                        \n",
    "                        if is_img_valid:\n",
    "                            processed_files.add(img_name)\n",
    "                        else:\n",
    "                            invalid_files.add(img_name)\n",
    "                            print(f\"图像 {img_name} 质量检查未通过\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"处理图像 {img_name} 时出错: {str(e)}\")\n",
    "                        error_files.add(img_name)\n",
    "                        # 为错误图像添加空预测\n",
    "                        predictions[img_name] = {\n",
    "                            \"label\": [],\n",
    "                            \"top\": [0],\n",
    "                            \"left\": [0],\n",
    "                            \"height\": [0],\n",
    "                            \"width\": [0]\n",
    "                        }\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"批次 {batch_idx} 处理出错: {str(e)}\")\n",
    "                for img_name in img_names:\n",
    "                    error_files.add(img_name)\n",
    "                    predictions[img_name] = {\n",
    "                        \"label\": [],\n",
    "                        \"top\": [0],\n",
    "                        \"left\": [0],\n",
    "                        \"height\": [0],\n",
    "                        \"width\": [0]\n",
    "                    }\n",
    "    \n",
    "    # 检查是否有遗漏的图片\n",
    "    missing_files = set(all_image_files) - processed_files - invalid_files - error_files\n",
    "    if missing_files:\n",
    "        print(f\"\\n警告：有 {len(missing_files)} 张图片未被处理：\")\n",
    "        for f in sorted(missing_files, key=lambda x: int(x.split('.')[0])):\n",
    "            print(f\"  {f}\")\n",
    "            # 为未处理的图片添加空预测\n",
    "            predictions[f] = {\n",
    "                \"label\": [],\n",
    "                \"top\": [0],\n",
    "                \"left\": [0],\n",
    "                \"height\": [0],\n",
    "                \"width\": [0]\n",
    "            }\n",
    "    \n",
    "    # 保存预测结果\n",
    "    print(\"保存预测结果...\")\n",
    "    with open(os.path.join(output_dir, \"predictions.json\"), \"w\") as f:\n",
    "        json.dump(predictions, f)\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"\\n测试完成！统计信息：\")\n",
    "    print(f\"总图片数: {len(all_image_files)}\")\n",
    "    print(f\"成功预测: {len(processed_files)}\")\n",
    "    print(f\"质量不合格: {len(invalid_files)}\")\n",
    "    print(f\"处理出错: {len(error_files)}\")\n",
    "    print(f\"未处理: {len(missing_files)}\")\n",
    "    \n",
    "    # 保存统计信息\n",
    "    stats = {\n",
    "        \"total_images\": len(all_image_files),\n",
    "        \"successful_predictions\": len(processed_files),\n",
    "        \"invalid_quality\": len(invalid_files),\n",
    "        \"processing_errors\": len(error_files),\n",
    "        \"missing_files\": len(missing_files),\n",
    "        \"invalid_files\": list(invalid_files),\n",
    "        \"error_files\": list(error_files),\n",
    "        \"missing_files_list\": list(missing_files)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"stats.json\"), \"w\") as f:\n",
    "        json.dump(stats, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b5081",
   "metadata": {},
   "source": [
    "生成csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def generate_csv(pred_file, test_dir, output_file):\n",
    "    # 读取预测结果\n",
    "    with open(pred_file, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    # 获取所有测试图片并排序\n",
    "    all_images = sorted(\n",
    "        [f for f in os.listdir(test_dir) if f.endswith('.png')],\n",
    "        key=lambda x: int(x.split('.')[0])\n",
    "    )\n",
    "    \n",
    "    # 创建输出目录（如果不存在）\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # 写入CSV文件\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # 写入表头\n",
    "        writer.writerow(['file_name', 'file_code'])\n",
    "        \n",
    "        # 写入每一行数据\n",
    "        processed_count = 0\n",
    "        empty_count = 0\n",
    "        \n",
    "        for img_name in all_images:\n",
    "            if img_name in predictions:\n",
    "                # 将标签列表转换为字符串\n",
    "                code = ''.join([str(l) for l in predictions[img_name]['label']]) if predictions[img_name]['label'] else ''\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                code = ''\n",
    "                empty_count += 1\n",
    "            writer.writerow([img_name, code])\n",
    "    \n",
    "    print(f'CSV文件已生成：{output_file}')\n",
    "    print(f'总图片数：{len(all_images)}')\n",
    "    print(f'有预测结果：{processed_count} 张')\n",
    "    print(f'无预测结果：{empty_count} 张')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pred_file = '../prediction_result/predictions.json'\n",
    "    test_dir = '../tcdata/mchar_test_a'\n",
    "    output_file = '../prediction_result/result.csv'\n",
    "    \n",
    "    generate_csv(pred_file, test_dir, output_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3cba6",
   "metadata": {},
   "source": [
    "预测结果检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c038220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 读取预测结果\n",
    "with open('../prediction_result/predictions.json', 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# 显示第一个预测结果的详细信息\n",
    "first_img = list(predictions.keys())[0]\n",
    "print(f\"第一张图片 {first_img} 的预测结果：\")\n",
    "print(json.dumps(predictions[first_img], indent=2, ensure_ascii=False))\n",
    "\n",
    "# 显示前5张图片的边界框数量\n",
    "print(\"\\n前5张图片的边界框数量：\")\n",
    "for i, (img_name, pred) in enumerate(list(predictions.items())[:5]):\n",
    "    print(f\"{img_name}: {len(pred['left'])} 个边界框，标签：{pred['label']}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d43af",
   "metadata": {},
   "source": [
    "可视化预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def visualize_predictions(test_dir, pred_file, output_dir, num_samples=10):\n",
    "    # 读取预测结果\n",
    "    with open(pred_file, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 设置颜色\n",
    "    box_color = (0, 255, 0)  # 绿色边界框\n",
    "    text_color = (255, 255, 255)  # 白色文字\n",
    "    \n",
    "    # 获取前num_samples个预测结果\n",
    "    for i, (img_name, pred) in enumerate(list(predictions.items())[:num_samples]):\n",
    "        # 读取原始图像\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        # 获取边界框和标签\n",
    "        x = pred['left'][0]\n",
    "        y = pred['top'][0]\n",
    "        w = pred['width'][0]\n",
    "        h = pred['height'][0]\n",
    "        labels = pred['label']\n",
    "        \n",
    "        # 绘制边界框\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), box_color, 2)\n",
    "        \n",
    "        # 将标签转换为字符串\n",
    "        label_str = ''.join([str(l) for l in labels])\n",
    "        \n",
    "        # 计算文本大小以优化显示位置\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(\n",
    "            label_str, cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)\n",
    "        \n",
    "        # 添加文本背景\n",
    "        cv2.rectangle(image, \n",
    "                     (x, y - text_h - 10), \n",
    "                     (x + text_w, y),\n",
    "                     box_color, -1)  # -1 表示填充矩形\n",
    "        \n",
    "        # 添加标签文本\n",
    "        cv2.putText(image, label_str, \n",
    "                    (x, y - 5), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.9, text_color, 2)\n",
    "        \n",
    "        # 保存结果\n",
    "        output_path = os.path.join(output_dir, f'vis_{img_name}')\n",
    "        cv2.imwrite(output_path, image)\n",
    "        \n",
    "        print(f'已处理图片 {i+1}/{num_samples}: {img_name}，检测到数字：{label_str}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_dir = '../tcdata/mchar_test_a'\n",
    "    pred_file = '../prediction_result/predictions.json'\n",
    "    output_dir = '../visualization_result'\n",
    "    \n",
    "    visualize_predictions(test_dir, pred_file, output_dir, num_samples=10)\n",
    "    print('可视化完成！结果保存在:', output_dir) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
